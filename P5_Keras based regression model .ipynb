{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras based regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a id='partA'></a>\n",
    "## Part A - Building a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data into predictors and target**\n",
    "\n",
    "- The target variable in this problem is the concrete sample strength. \n",
    "\n",
    "\n",
    "- The predictors will be all the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTORS: \n",
      "\n",
      "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   Coarse Aggregate  Fine Aggregate  Age  \n",
      "0            1040.0           676.0   28  \n",
      "1            1055.0           676.0   28  \n",
      "2             932.0           594.0  270  \n",
      "3             932.0           594.0  365  \n",
      "4             978.4           825.5  360  \n",
      "\n",
      "\n",
      "TARGET: \n",
      "\n",
      "0    79.99\n",
      "1    61.89\n",
      "2    40.27\n",
      "3    41.05\n",
      "4    44.30\n",
      "Name: Strength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into predictors and target\n",
    "concrete_data_col = concrete_data.columns\n",
    "\n",
    "# defining predictors by including all feature columns except strength.\n",
    "predictors = concrete_data[concrete_data_col[concrete_data_col != 'Strength']]\n",
    "\n",
    "# defining the target column\n",
    "target = concrete_data['Strength']\n",
    "\n",
    "print('PREDICTORS: \\n')\n",
    "print(predictors.head()) \n",
    "print('\\n')\n",
    "print('TARGET: \\n')\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now render the input one-dimensional for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rendering input one-dimensional\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I now build the neural network using the Keras library.\n",
    "\n",
    "\n",
    "- I include **one hidden layer** with **10 nodes**. \n",
    "\n",
    "\n",
    "- I include a **ReLU** activation function.\n",
    "\n",
    "\n",
    "- I compile the model with the **adam** optimizer and the **mean_squared_error** as the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1315b1a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the model\n",
    "def regr_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu', input_shape = (n_cols, )))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compiling the model\n",
    "    model.compile(optimizer = 'adam', \n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics = ['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = regr_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I now split the data.\n",
    "\n",
    "\n",
    "- The simplest way is via Scikit-Learn.\n",
    "\n",
    "\n",
    "- I import **train_test_split** from **sklearn.model_selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation (i.e: test) data: \n",
      "\n",
      "(309, 8)\n",
      "(309,) \n",
      "\n",
      "The training data: \n",
      "\n",
      "(721, 8)\n",
      "(721,)\n"
     ]
    }
   ],
   "source": [
    "# randomly splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, \n",
    "                                                    target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state = 4)\n",
    "\n",
    "print('The validation (i.e: test) data: \\n')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape, '\\n')\n",
    "\n",
    "print('The training data: \\n')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I now use the .fit method to fit the model for training.\n",
    "\n",
    "\n",
    "- **Note**: One may use 'shuffle = True' to shuffle data again when training in each epochs but I did not find any major difference at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 48.7318 - mse: 48.7318 - val_loss: 52.4135 - val_mse: 52.4135\n",
      "Epoch 2/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 50.4573 - mse: 50.4573 - val_loss: 51.6557 - val_mse: 51.6557\n",
      "Epoch 3/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 48.8683 - mse: 48.8683 - val_loss: 49.5602 - val_mse: 49.5602\n",
      "Epoch 4/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 45.7215 - mse: 45.7215 - val_loss: 49.4905 - val_mse: 49.4905\n",
      "Epoch 5/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 48.1456 - mse: 48.1456 - val_loss: 46.7803 - val_mse: 46.7803\n",
      "Epoch 6/50\n",
      "721/721 [==============================] - 0s 65us/step - loss: 48.3744 - mse: 48.3744 - val_loss: 49.2696 - val_mse: 49.2696\n",
      "Epoch 7/50\n",
      "721/721 [==============================] - 0s 68us/step - loss: 45.9735 - mse: 45.9735 - val_loss: 45.4469 - val_mse: 45.4469\n",
      "Epoch 8/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 46.8620 - mse: 46.8620 - val_loss: 45.4127 - val_mse: 45.4127\n",
      "Epoch 9/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 47.1900 - mse: 47.1900 - val_loss: 47.8959 - val_mse: 47.8959\n",
      "Epoch 10/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 45.8289 - mse: 45.8289 - val_loss: 45.4504 - val_mse: 45.4504\n",
      "Epoch 11/50\n",
      "721/721 [==============================] - 0s 65us/step - loss: 46.4531 - mse: 46.4531 - val_loss: 45.7307 - val_mse: 45.7307\n",
      "Epoch 12/50\n",
      "721/721 [==============================] - 0s 82us/step - loss: 46.8945 - mse: 46.8945 - val_loss: 47.7168 - val_mse: 47.7168\n",
      "Epoch 13/50\n",
      "721/721 [==============================] - 0s 75us/step - loss: 46.2880 - mse: 46.2880 - val_loss: 46.3554 - val_mse: 46.3554\n",
      "Epoch 14/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 48.6856 - mse: 48.6856 - val_loss: 48.8908 - val_mse: 48.8908\n",
      "Epoch 15/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 48.2443 - mse: 48.2444 - val_loss: 44.9593 - val_mse: 44.9593\n",
      "Epoch 16/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 46.4078 - mse: 46.4078 - val_loss: 55.0158 - val_mse: 55.0158\n",
      "Epoch 17/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 48.7170 - mse: 48.7170 - val_loss: 49.8467 - val_mse: 49.8467\n",
      "Epoch 18/50\n",
      "721/721 [==============================] - 0s 92us/step - loss: 50.3538 - mse: 50.3538 - val_loss: 51.3249 - val_mse: 51.3249\n",
      "Epoch 19/50\n",
      "721/721 [==============================] - 0s 96us/step - loss: 48.0220 - mse: 48.0220 - val_loss: 46.1620 - val_mse: 46.1620\n",
      "Epoch 20/50\n",
      "721/721 [==============================] - 0s 103us/step - loss: 45.8643 - mse: 45.8643 - val_loss: 48.7521 - val_mse: 48.7521\n",
      "Epoch 21/50\n",
      "721/721 [==============================] - 0s 68us/step - loss: 46.4533 - mse: 46.4533 - val_loss: 45.4287 - val_mse: 45.4287\n",
      "Epoch 22/50\n",
      "721/721 [==============================] - 0s 79us/step - loss: 47.8332 - mse: 47.8332 - val_loss: 51.1366 - val_mse: 51.1366\n",
      "Epoch 23/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 47.1164 - mse: 47.1164 - val_loss: 46.2002 - val_mse: 46.2002\n",
      "Epoch 24/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 47.0255 - mse: 47.0255 - val_loss: 48.8658 - val_mse: 48.8658\n",
      "Epoch 25/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 47.3399 - mse: 47.3399 - val_loss: 45.3814 - val_mse: 45.3814\n",
      "Epoch 26/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 46.8583 - mse: 46.8583 - val_loss: 47.4469 - val_mse: 47.4469\n",
      "Epoch 27/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 49.0375 - mse: 49.0375 - val_loss: 46.4748 - val_mse: 46.4748\n",
      "Epoch 28/50\n",
      "721/721 [==============================] - 0s 87us/step - loss: 47.4360 - mse: 47.4360 - val_loss: 47.9118 - val_mse: 47.9118\n",
      "Epoch 29/50\n",
      "721/721 [==============================] - 0s 113us/step - loss: 47.4997 - mse: 47.4997 - val_loss: 45.8759 - val_mse: 45.8759\n",
      "Epoch 30/50\n",
      "721/721 [==============================] - 0s 81us/step - loss: 45.9002 - mse: 45.9002 - val_loss: 45.1008 - val_mse: 45.1008\n",
      "Epoch 31/50\n",
      "721/721 [==============================] - 0s 77us/step - loss: 48.8139 - mse: 48.8139 - val_loss: 54.5293 - val_mse: 54.5293\n",
      "Epoch 32/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 49.2242 - mse: 49.2242 - val_loss: 45.0227 - val_mse: 45.0227\n",
      "Epoch 33/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 46.3086 - mse: 46.3086 - val_loss: 45.1662 - val_mse: 45.1662\n",
      "Epoch 34/50\n",
      "721/721 [==============================] - 0s 68us/step - loss: 45.3727 - mse: 45.3727 - val_loss: 50.2082 - val_mse: 50.2082\n",
      "Epoch 35/50\n",
      "721/721 [==============================] - 0s 89us/step - loss: 46.7268 - mse: 46.7268 - val_loss: 47.0476 - val_mse: 47.0476\n",
      "Epoch 36/50\n",
      "721/721 [==============================] - 0s 86us/step - loss: 49.9380 - mse: 49.9380 - val_loss: 46.7047 - val_mse: 46.7047\n",
      "Epoch 37/50\n",
      "721/721 [==============================] - 0s 72us/step - loss: 46.3361 - mse: 46.3361 - val_loss: 46.4613 - val_mse: 46.4613\n",
      "Epoch 38/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 46.3309 - mse: 46.3309 - val_loss: 46.4971 - val_mse: 46.4972\n",
      "Epoch 39/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 48.0117 - mse: 48.0117 - val_loss: 46.0373 - val_mse: 46.0373\n",
      "Epoch 40/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 46.4542 - mse: 46.4542 - val_loss: 51.1958 - val_mse: 51.1958\n",
      "Epoch 41/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 47.4069 - mse: 47.4069 - val_loss: 46.8137 - val_mse: 46.8137\n",
      "Epoch 42/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 47.1237 - mse: 47.1237 - val_loss: 46.0936 - val_mse: 46.0936\n",
      "Epoch 43/50\n",
      "721/721 [==============================] - 0s 60us/step - loss: 46.0756 - mse: 46.0756 - val_loss: 51.1649 - val_mse: 51.1649\n",
      "Epoch 44/50\n",
      "721/721 [==============================] - 0s 60us/step - loss: 46.5443 - mse: 46.5443 - val_loss: 48.8691 - val_mse: 48.8691\n",
      "Epoch 45/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 46.9945 - mse: 46.9945 - val_loss: 45.0050 - val_mse: 45.0050\n",
      "Epoch 46/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 45.7280 - mse: 45.7280 - val_loss: 47.2340 - val_mse: 47.2340\n",
      "Epoch 47/50\n",
      "721/721 [==============================] - 0s 59us/step - loss: 46.1752 - mse: 46.1752 - val_loss: 46.5953 - val_mse: 46.5953\n",
      "Epoch 48/50\n",
      "721/721 [==============================] - 0s 60us/step - loss: 46.5873 - mse: 46.5873 - val_loss: 45.8121 - val_mse: 45.8121\n",
      "Epoch 49/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 46.1369 - mse: 46.1369 - val_loss: 44.9782 - val_mse: 44.9782\n",
      "Epoch 50/50\n",
      "721/721 [==============================] - 0s 121us/step - loss: 45.5677 - mse: 45.5677 - val_loss: 45.6010 - val_mse: 45.6010\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, \n",
    "                      y_train,\n",
    "                      validation_data = [X_test, y_test],\n",
    "                      epochs = 50,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I proceed to now evaluate the model with the .predict method in a first instance.\n",
    "\n",
    "\n",
    "- I label the predicted values by **y_pred**\n",
    "\n",
    "\n",
    "- I note that **y_pred** is not of the same shape as y_test, so I ensure that y_test and y_pred have the same shape before implementing any evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.327934]\n",
      " [53.799763]\n",
      " [25.145731]\n",
      " [42.86279 ]\n",
      " [44.69525 ]\n",
      " [42.6908  ]\n",
      " [52.20326 ]\n",
      " [24.99718 ]\n",
      " [59.718204]\n",
      " [22.357647]]\n",
      "\n",
      "\n",
      "Shape of array y_pred is: (309, 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_pred is:', y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.52]\n",
      " [50.53]\n",
      " [21.82]\n",
      " [38.8 ]\n",
      " [55.6 ]\n",
      " [39.42]\n",
      " [81.75]\n",
      " [24.5 ]\n",
      " [69.84]\n",
      " [19.99]]\n",
      "\n",
      "\n",
      "Shape of array y_testarray is: (309, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping array\n",
    "import numpy as np\n",
    "y_testarray = np.transpose(([y_test]))\n",
    "\n",
    "print(y_testarray[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_testarray is:', y_testarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.601026865748956"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_testarray, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.7318,\n",
       " 50.457325,\n",
       " 48.868332,\n",
       " 45.721493,\n",
       " 48.14556,\n",
       " 48.37437,\n",
       " 45.97351,\n",
       " 46.862045,\n",
       " 47.19001,\n",
       " 45.82891,\n",
       " 46.45313,\n",
       " 46.894463,\n",
       " 46.287964,\n",
       " 48.685593,\n",
       " 48.24435,\n",
       " 46.40776,\n",
       " 48.716995,\n",
       " 50.353767,\n",
       " 48.02197,\n",
       " 45.864338,\n",
       " 46.45332,\n",
       " 47.833202,\n",
       " 47.116405,\n",
       " 47.02548,\n",
       " 47.339943,\n",
       " 46.85834,\n",
       " 49.03749,\n",
       " 47.435978,\n",
       " 47.499664,\n",
       " 45.900238,\n",
       " 48.813908,\n",
       " 49.224174,\n",
       " 46.308567,\n",
       " 45.37275,\n",
       " 46.726833,\n",
       " 49.937973,\n",
       " 46.336105,\n",
       " 46.33087,\n",
       " 48.011654,\n",
       " 46.45423,\n",
       " 47.406853,\n",
       " 47.123745,\n",
       " 46.07556,\n",
       " 46.54427,\n",
       " 46.99451,\n",
       " 45.72799,\n",
       " 46.175213,\n",
       " 46.587303,\n",
       " 46.136856,\n",
       " 45.567722]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the mean squared error for each 50 epochs\n",
    "mse = model_fit.history['mse']\n",
    "\n",
    "# creating the list of the 50 individual mse\n",
    "list(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the 50 mse =  47.248817\n",
      "Standard deviation of the 50 mse = 1.2687162\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(mse)\n",
    "st_dev = np.std(mse)\n",
    "\n",
    "print('Mean of the 50 mse = ', mean)\n",
    "print('Standard deviation of the 50 mse =', st_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a id ='partB'></a>\n",
    "## Part B - Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate       Age  \n",
       "0          0.862735       -1.217079 -0.279597  \n",
       "1          1.055651       -1.217079 -0.279597  \n",
       "2         -0.526262       -2.239829  3.551340  \n",
       "3         -0.526262       -2.239829  5.055221  \n",
       "4          0.070492        0.647569  4.976069  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing the data\n",
    "# We subtract the mean and divide by standard deviation\n",
    "pred_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "pred_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I repeat Part A. \n",
    "\n",
    "\n",
    "- I specify the input. \n",
    "\n",
    "\n",
    "- I then build the model, split the validation and training data, and fit the model.\n",
    "\n",
    "\n",
    "- I then compare the new mean of the new individual mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the number of predictors in n_cols_norm as 1-dim inputs\n",
    "n_cols_norm = pred_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x10e48ea90>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the model\n",
    "def regr_model_norm():\n",
    "    model_norm = Sequential()\n",
    "    model_norm.add(Dense(10, activation = 'relu', input_shape = (n_cols_norm, )))\n",
    "    model_norm.add(Dense(10, activation = 'relu'))\n",
    "    model_norm.add(Dense(1))\n",
    "    \n",
    "    # compiling the model\n",
    "    model_norm.compile(optimizer = 'adam', \n",
    "                       loss = 'mean_squared_error',\n",
    "                       metrics = ['mse'])\n",
    "    \n",
    "    return model_norm\n",
    "\n",
    "model_norm = regr_model_norm()\n",
    "model_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation (i.e: test) data: \n",
      "\n",
      "(309, 8)\n",
      "(309,) \n",
      "\n",
      "The training data: \n",
      "\n",
      "(721, 8)\n",
      "(721,)\n"
     ]
    }
   ],
   "source": [
    "# split the validation and training data randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(pred_norm, \n",
    "                                                        target, \n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state = 4)\n",
    "\n",
    "print('The validation (i.e: test) data: \\n')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape, '\\n')\n",
    "\n",
    "print('The training data: \\n')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/50\n",
      "721/721 [==============================] - 0s 84us/step - loss: 27.0493 - mse: 27.0493 - val_loss: 38.4258 - val_mse: 38.4258\n",
      "Epoch 2/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 27.1468 - mse: 27.1468 - val_loss: 38.3502 - val_mse: 38.3502\n",
      "Epoch 3/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 26.9829 - mse: 26.9829 - val_loss: 38.4202 - val_mse: 38.4202\n",
      "Epoch 4/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 27.2080 - mse: 27.2080 - val_loss: 38.4493 - val_mse: 38.4493\n",
      "Epoch 5/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.1331 - mse: 27.1331 - val_loss: 38.5869 - val_mse: 38.5869\n",
      "Epoch 6/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.0149 - mse: 27.0149 - val_loss: 38.4049 - val_mse: 38.4049\n",
      "Epoch 7/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 27.3796 - mse: 27.3796 - val_loss: 38.6787 - val_mse: 38.6787\n",
      "Epoch 8/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.1511 - mse: 27.1511 - val_loss: 38.3840 - val_mse: 38.3840\n",
      "Epoch 9/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.1938 - mse: 27.1938 - val_loss: 38.4874 - val_mse: 38.4874\n",
      "Epoch 10/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 27.1584 - mse: 27.1584 - val_loss: 38.4487 - val_mse: 38.4487\n",
      "Epoch 11/50\n",
      "721/721 [==============================] - 0s 64us/step - loss: 27.2434 - mse: 27.2434 - val_loss: 38.4391 - val_mse: 38.4391\n",
      "Epoch 12/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 27.3866 - mse: 27.3866 - val_loss: 38.5173 - val_mse: 38.5173\n",
      "Epoch 13/50\n",
      "721/721 [==============================] - 0s 59us/step - loss: 27.4833 - mse: 27.4833 - val_loss: 38.4508 - val_mse: 38.4508\n",
      "Epoch 14/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 27.3113 - mse: 27.3114 - val_loss: 38.6889 - val_mse: 38.6889\n",
      "Epoch 15/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.0877 - mse: 27.0877 - val_loss: 38.4833 - val_mse: 38.4833\n",
      "Epoch 16/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 27.2371 - mse: 27.2371 - val_loss: 38.4380 - val_mse: 38.4380\n",
      "Epoch 17/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 27.2222 - mse: 27.2222 - val_loss: 38.3183 - val_mse: 38.3183\n",
      "Epoch 18/50\n",
      "721/721 [==============================] - 0s 68us/step - loss: 27.0799 - mse: 27.0799 - val_loss: 38.3835 - val_mse: 38.3835\n",
      "Epoch 19/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.9841 - mse: 26.9841 - val_loss: 38.3985 - val_mse: 38.3985\n",
      "Epoch 20/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 27.1389 - mse: 27.1389 - val_loss: 38.4328 - val_mse: 38.4328\n",
      "Epoch 21/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.9766 - mse: 26.9766 - val_loss: 38.1879 - val_mse: 38.1879\n",
      "Epoch 22/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 27.1011 - mse: 27.1011 - val_loss: 38.4377 - val_mse: 38.4377\n",
      "Epoch 23/50\n",
      "721/721 [==============================] - 0s 65us/step - loss: 27.0698 - mse: 27.0698 - val_loss: 38.2491 - val_mse: 38.2491\n",
      "Epoch 24/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 27.1097 - mse: 27.1097 - val_loss: 38.1655 - val_mse: 38.1656\n",
      "Epoch 25/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.9487 - mse: 26.9487 - val_loss: 38.2837 - val_mse: 38.2837\n",
      "Epoch 26/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 26.8241 - mse: 26.8241 - val_loss: 38.2010 - val_mse: 38.2010\n",
      "Epoch 27/50\n",
      "721/721 [==============================] - 0s 68us/step - loss: 26.9863 - mse: 26.9863 - val_loss: 38.2015 - val_mse: 38.2015\n",
      "Epoch 28/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 26.8810 - mse: 26.8809 - val_loss: 38.3867 - val_mse: 38.3867\n",
      "Epoch 29/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.8912 - mse: 26.8912 - val_loss: 38.1544 - val_mse: 38.1544\n",
      "Epoch 30/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 27.4045 - mse: 27.4045 - val_loss: 38.3877 - val_mse: 38.3877\n",
      "Epoch 31/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 27.1748 - mse: 27.1748 - val_loss: 38.3368 - val_mse: 38.3368\n",
      "Epoch 32/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.8789 - mse: 26.8789 - val_loss: 38.2476 - val_mse: 38.2476\n",
      "Epoch 33/50\n",
      "721/721 [==============================] - 0s 79us/step - loss: 26.9773 - mse: 26.9773 - val_loss: 38.2875 - val_mse: 38.2875\n",
      "Epoch 34/50\n",
      "721/721 [==============================] - 0s 85us/step - loss: 26.9121 - mse: 26.9121 - val_loss: 38.1540 - val_mse: 38.1540\n",
      "Epoch 35/50\n",
      "721/721 [==============================] - 0s 83us/step - loss: 26.9964 - mse: 26.9964 - val_loss: 38.0940 - val_mse: 38.0940\n",
      "Epoch 36/50\n",
      "721/721 [==============================] - 0s 73us/step - loss: 26.6868 - mse: 26.6868 - val_loss: 38.2139 - val_mse: 38.2139\n",
      "Epoch 37/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 26.8456 - mse: 26.8456 - val_loss: 38.1787 - val_mse: 38.1787\n",
      "Epoch 38/50\n",
      "721/721 [==============================] - 0s 63us/step - loss: 26.8976 - mse: 26.8975 - val_loss: 38.1351 - val_mse: 38.1351\n",
      "Epoch 39/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 26.8114 - mse: 26.8114 - val_loss: 38.4925 - val_mse: 38.4925\n",
      "Epoch 40/50\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.8240 - mse: 26.8240 - val_loss: 38.0596 - val_mse: 38.0596\n",
      "Epoch 41/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 27.1126 - mse: 27.1126 - val_loss: 38.1243 - val_mse: 38.1243\n",
      "Epoch 42/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.7915 - mse: 26.7915 - val_loss: 38.2108 - val_mse: 38.2108\n",
      "Epoch 43/50\n",
      "721/721 [==============================] - 0s 66us/step - loss: 26.6696 - mse: 26.6696 - val_loss: 38.0217 - val_mse: 38.0217\n",
      "Epoch 44/50\n",
      "721/721 [==============================] - 0s 85us/step - loss: 26.7554 - mse: 26.7554 - val_loss: 38.1414 - val_mse: 38.1414\n",
      "Epoch 45/50\n",
      "721/721 [==============================] - 0s 120us/step - loss: 26.8017 - mse: 26.8017 - val_loss: 38.2031 - val_mse: 38.2031\n",
      "Epoch 46/50\n",
      "721/721 [==============================] - 0s 115us/step - loss: 26.7205 - mse: 26.7205 - val_loss: 38.0850 - val_mse: 38.0850\n",
      "Epoch 47/50\n",
      "721/721 [==============================] - 0s 78us/step - loss: 26.8147 - mse: 26.8147 - val_loss: 38.0871 - val_mse: 38.0871\n",
      "Epoch 48/50\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.9149 - mse: 26.9149 - val_loss: 38.4309 - val_mse: 38.4309\n",
      "Epoch 49/50\n",
      "721/721 [==============================] - 0s 61us/step - loss: 26.9851 - mse: 26.9851 - val_loss: 38.1882 - val_mse: 38.1882\n",
      "Epoch 50/50\n",
      "721/721 [==============================] - 0s 91us/step - loss: 26.7554 - mse: 26.7554 - val_loss: 38.0846 - val_mse: 38.0846\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "model_norm_fit = model_norm.fit(X_train1, \n",
    "                      y_train1,\n",
    "                      validation_data = [X_test1, y_test1],\n",
    "                      epochs = 50,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47.03498 ]\n",
      " [51.693844]\n",
      " [20.899956]\n",
      " [37.824623]\n",
      " [52.644394]\n",
      " [43.076744]\n",
      " [56.85761 ]\n",
      " [24.296974]\n",
      " [59.39751 ]\n",
      " [28.411846]]\n",
      "\n",
      "\n",
      "Shape of array y_pred is: (309, 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model with normalised data\n",
    "y_pred1 = model_norm.predict(X_test1)\n",
    "\n",
    "print(y_pred1[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_pred is:', y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.52]\n",
      " [50.53]\n",
      " [21.82]\n",
      " [38.8 ]\n",
      " [55.6 ]\n",
      " [39.42]\n",
      " [81.75]\n",
      " [24.5 ]\n",
      " [69.84]\n",
      " [19.99]]\n",
      "\n",
      "\n",
      "Shape of array y_testarray1 is: (309, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping array y_test1\n",
    "y_testarray1 = np.transpose(([y_test1]))\n",
    "\n",
    "print(y_testarray1[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_testarray1 is:', y_testarray1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.08460959710628"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating new mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_testarray1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.049284,\n",
       " 27.146837,\n",
       " 26.982853,\n",
       " 27.20802,\n",
       " 27.133114,\n",
       " 27.01495,\n",
       " 27.37964,\n",
       " 27.151093,\n",
       " 27.19383,\n",
       " 27.158417,\n",
       " 27.243437,\n",
       " 27.386585,\n",
       " 27.483265,\n",
       " 27.311352,\n",
       " 27.087736,\n",
       " 27.237076,\n",
       " 27.222233,\n",
       " 27.07987,\n",
       " 26.984148,\n",
       " 27.138916,\n",
       " 26.97658,\n",
       " 27.101086,\n",
       " 27.069777,\n",
       " 27.10966,\n",
       " 26.948702,\n",
       " 26.824123,\n",
       " 26.986301,\n",
       " 26.880949,\n",
       " 26.891207,\n",
       " 27.40449,\n",
       " 27.1748,\n",
       " 26.878946,\n",
       " 26.977251,\n",
       " 26.912052,\n",
       " 26.996416,\n",
       " 26.686817,\n",
       " 26.8456,\n",
       " 26.897549,\n",
       " 26.81136,\n",
       " 26.824034,\n",
       " 27.112597,\n",
       " 26.791473,\n",
       " 26.669579,\n",
       " 26.755426,\n",
       " 26.801664,\n",
       " 26.72053,\n",
       " 26.814726,\n",
       " 26.91487,\n",
       " 26.985079,\n",
       " 26.75542]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the mean squared error for each 50 epochs in new model\n",
    "mse_norm_data = model_norm_fit.history['mse']\n",
    "\n",
    "# creating the list of the 50 individual mse\n",
    "list(mse_norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the 50 mse with normalized data =  27.022232\n",
      "Standard deviation of the 50 mse with normalized data = 0.19578068\n"
     ]
    }
   ],
   "source": [
    "mean_norm = np.mean(mse_norm_data)\n",
    "st_dev_norm = np.std(mse_norm_data)\n",
    "\n",
    "print('Mean of the 50 mse with normalized data = ', mean_norm)\n",
    "print('Standard deviation of the 50 mse with normalized data =', st_dev_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean and standard deviation of the new results are even smaller indicating that the algorithm is performing much better already with the normalized data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a id = 'partC'></a>\n",
    "## Part C - Increasing the number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We repeat Part B but with increased number of epochs. \n",
    "\n",
    "\n",
    "- **Note**: I shall not redefine the variables train and test since I am using the same normalised training and testing data. However, as the predictions are expected to be different, it makes sense to relabel our prediction data as, say **y_pred2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.0604 - mse: 26.0604 - val_loss: 41.5808 - val_mse: 41.5808\n",
      "Epoch 2/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 26.2958 - mse: 26.2958 - val_loss: 41.8692 - val_mse: 41.8692\n",
      "Epoch 3/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.2135 - mse: 26.2135 - val_loss: 41.7550 - val_mse: 41.7550\n",
      "Epoch 4/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.2839 - mse: 26.2839 - val_loss: 41.7346 - val_mse: 41.7346\n",
      "Epoch 5/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.1682 - mse: 26.1682 - val_loss: 41.7237 - val_mse: 41.7237\n",
      "Epoch 6/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.1372 - mse: 26.1372 - val_loss: 41.6211 - val_mse: 41.6211\n",
      "Epoch 7/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.3005 - mse: 26.3005 - val_loss: 41.8603 - val_mse: 41.8603\n",
      "Epoch 8/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 26.2533 - mse: 26.2533 - val_loss: 41.5407 - val_mse: 41.5407\n",
      "Epoch 9/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.1954 - mse: 26.1954 - val_loss: 41.6808 - val_mse: 41.6808\n",
      "Epoch 10/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 26.0047 - mse: 26.0047 - val_loss: 41.6981 - val_mse: 41.6981\n",
      "Epoch 11/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.1660 - mse: 26.1660 - val_loss: 41.4876 - val_mse: 41.4876\n",
      "Epoch 12/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 26.3001 - mse: 26.3001 - val_loss: 41.9265 - val_mse: 41.9265\n",
      "Epoch 13/100\n",
      "721/721 [==============================] - 0s 63us/step - loss: 26.1910 - mse: 26.1910 - val_loss: 41.7384 - val_mse: 41.7384\n",
      "Epoch 14/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.1506 - mse: 26.1506 - val_loss: 41.5565 - val_mse: 41.5565\n",
      "Epoch 15/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 26.2137 - mse: 26.2137 - val_loss: 42.0750 - val_mse: 42.0750\n",
      "Epoch 16/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 26.0396 - mse: 26.0396 - val_loss: 41.3825 - val_mse: 41.3825\n",
      "Epoch 17/100\n",
      "721/721 [==============================] - 0s 66us/step - loss: 26.2998 - mse: 26.2998 - val_loss: 42.3073 - val_mse: 42.3073\n",
      "Epoch 18/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 26.2393 - mse: 26.2393 - val_loss: 41.7998 - val_mse: 41.7998\n",
      "Epoch 19/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 26.0352 - mse: 26.0352 - val_loss: 41.4070 - val_mse: 41.4070\n",
      "Epoch 20/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 25.9682 - mse: 25.9682 - val_loss: 41.6704 - val_mse: 41.6704\n",
      "Epoch 21/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 25.9837 - mse: 25.9837 - val_loss: 41.5179 - val_mse: 41.5179\n",
      "Epoch 22/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 26.0618 - mse: 26.0618 - val_loss: 41.6032 - val_mse: 41.6032\n",
      "Epoch 23/100\n",
      "721/721 [==============================] - 0s 66us/step - loss: 26.1196 - mse: 26.1196 - val_loss: 41.4555 - val_mse: 41.4555\n",
      "Epoch 24/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 25.9637 - mse: 25.9637 - val_loss: 41.6083 - val_mse: 41.6083\n",
      "Epoch 25/100\n",
      "721/721 [==============================] - 0s 66us/step - loss: 26.0836 - mse: 26.0836 - val_loss: 41.4476 - val_mse: 41.4476\n",
      "Epoch 26/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 26.1222 - mse: 26.1222 - val_loss: 41.7088 - val_mse: 41.7088\n",
      "Epoch 27/100\n",
      "721/721 [==============================] - 0s 63us/step - loss: 25.9746 - mse: 25.9746 - val_loss: 41.5393 - val_mse: 41.5393\n",
      "Epoch 28/100\n",
      "721/721 [==============================] - 0s 66us/step - loss: 25.8358 - mse: 25.8358 - val_loss: 41.3698 - val_mse: 41.3699\n",
      "Epoch 29/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 25.9600 - mse: 25.9600 - val_loss: 41.4488 - val_mse: 41.4488\n",
      "Epoch 30/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.0763 - mse: 26.0763 - val_loss: 41.3501 - val_mse: 41.3501\n",
      "Epoch 31/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 25.7936 - mse: 25.7936 - val_loss: 41.3544 - val_mse: 41.3544\n",
      "Epoch 32/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 25.9325 - mse: 25.9325 - val_loss: 41.5623 - val_mse: 41.5623\n",
      "Epoch 33/100\n",
      "721/721 [==============================] - 0s 63us/step - loss: 25.9723 - mse: 25.9723 - val_loss: 41.2398 - val_mse: 41.2398\n",
      "Epoch 34/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 25.8096 - mse: 25.8096 - val_loss: 41.4740 - val_mse: 41.4740\n",
      "Epoch 35/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.9411 - mse: 25.9411 - val_loss: 41.6341 - val_mse: 41.6341\n",
      "Epoch 36/100\n",
      "721/721 [==============================] - 0s 73us/step - loss: 25.8873 - mse: 25.8873 - val_loss: 41.2189 - val_mse: 41.2189\n",
      "Epoch 37/100\n",
      "721/721 [==============================] - 0s 66us/step - loss: 26.0262 - mse: 26.0262 - val_loss: 41.4251 - val_mse: 41.4251\n",
      "Epoch 38/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 25.8784 - mse: 25.8784 - val_loss: 41.3123 - val_mse: 41.3123\n",
      "Epoch 39/100\n",
      "721/721 [==============================] - 0s 65us/step - loss: 25.9667 - mse: 25.9667 - val_loss: 41.2323 - val_mse: 41.2323\n",
      "Epoch 40/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 25.8712 - mse: 25.8712 - val_loss: 41.3014 - val_mse: 41.3014\n",
      "Epoch 41/100\n",
      "721/721 [==============================] - 0s 81us/step - loss: 25.8861 - mse: 25.8861 - val_loss: 41.1997 - val_mse: 41.1997\n",
      "Epoch 42/100\n",
      "721/721 [==============================] - 0s 108us/step - loss: 25.8076 - mse: 25.8076 - val_loss: 41.3540 - val_mse: 41.3540\n",
      "Epoch 43/100\n",
      "721/721 [==============================] - 0s 117us/step - loss: 25.7939 - mse: 25.7939 - val_loss: 41.1734 - val_mse: 41.1734\n",
      "Epoch 44/100\n",
      "721/721 [==============================] - 0s 85us/step - loss: 25.8427 - mse: 25.8427 - val_loss: 41.2371 - val_mse: 41.2371\n",
      "Epoch 45/100\n",
      "721/721 [==============================] - 0s 88us/step - loss: 25.8186 - mse: 25.8185 - val_loss: 41.0257 - val_mse: 41.0257\n",
      "Epoch 46/100\n",
      "721/721 [==============================] - 0s 86us/step - loss: 25.7484 - mse: 25.7484 - val_loss: 41.5217 - val_mse: 41.5217\n",
      "Epoch 47/100\n",
      "721/721 [==============================] - 0s 109us/step - loss: 25.9000 - mse: 25.9000 - val_loss: 41.2572 - val_mse: 41.2572\n",
      "Epoch 48/100\n",
      "721/721 [==============================] - 0s 100us/step - loss: 25.8301 - mse: 25.8301 - val_loss: 41.2141 - val_mse: 41.2141\n",
      "Epoch 49/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 25.7861 - mse: 25.7861 - val_loss: 41.2331 - val_mse: 41.2331\n",
      "Epoch 50/100\n",
      "721/721 [==============================] - 0s 75us/step - loss: 25.8909 - mse: 25.8909 - val_loss: 41.1778 - val_mse: 41.1778\n",
      "Epoch 51/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.6810 - mse: 25.6810 - val_loss: 41.0674 - val_mse: 41.0674\n",
      "Epoch 52/100\n",
      "721/721 [==============================] - 0s 77us/step - loss: 25.6684 - mse: 25.6684 - val_loss: 41.0329 - val_mse: 41.0329\n",
      "Epoch 53/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.7064 - mse: 25.7064 - val_loss: 41.3121 - val_mse: 41.3121\n",
      "Epoch 54/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 25.6760 - mse: 25.6760 - val_loss: 40.9942 - val_mse: 40.9942\n",
      "Epoch 55/100\n",
      "721/721 [==============================] - 0s 63us/step - loss: 25.9033 - mse: 25.9033 - val_loss: 41.0902 - val_mse: 41.0902\n",
      "Epoch 56/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.9757 - mse: 25.9757 - val_loss: 41.3963 - val_mse: 41.3963\n",
      "Epoch 57/100\n",
      "721/721 [==============================] - 0s 72us/step - loss: 25.8722 - mse: 25.8722 - val_loss: 40.8190 - val_mse: 40.8190\n",
      "Epoch 58/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 25.7601 - mse: 25.7601 - val_loss: 40.8929 - val_mse: 40.8929\n",
      "Epoch 59/100\n",
      "721/721 [==============================] - 0s 83us/step - loss: 25.7305 - mse: 25.7305 - val_loss: 41.2714 - val_mse: 41.2714\n",
      "Epoch 60/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.7906 - mse: 25.7906 - val_loss: 40.9091 - val_mse: 40.9091\n",
      "Epoch 61/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 25.9170 - mse: 25.9170 - val_loss: 41.0187 - val_mse: 41.0187\n",
      "Epoch 62/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 25.6525 - mse: 25.6525 - val_loss: 41.1596 - val_mse: 41.1596\n",
      "Epoch 63/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.6808 - mse: 25.6808 - val_loss: 41.1460 - val_mse: 41.1460\n",
      "Epoch 64/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.6024 - mse: 25.6024 - val_loss: 40.6908 - val_mse: 40.6908\n",
      "Epoch 65/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.6297 - mse: 25.6297 - val_loss: 40.9951 - val_mse: 40.9951\n",
      "Epoch 66/100\n",
      "721/721 [==============================] - 0s 56us/step - loss: 25.6010 - mse: 25.6010 - val_loss: 40.9613 - val_mse: 40.9613\n",
      "Epoch 67/100\n",
      "721/721 [==============================] - 0s 56us/step - loss: 25.5720 - mse: 25.5720 - val_loss: 40.8691 - val_mse: 40.8691\n",
      "Epoch 68/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.6308 - mse: 25.6308 - val_loss: 40.7970 - val_mse: 40.7970\n",
      "Epoch 69/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 25.6442 - mse: 25.6442 - val_loss: 41.0956 - val_mse: 41.0956\n",
      "Epoch 70/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.5934 - mse: 25.5934 - val_loss: 40.8386 - val_mse: 40.8386\n",
      "Epoch 71/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.6967 - mse: 25.6967 - val_loss: 40.8364 - val_mse: 40.8364\n",
      "Epoch 72/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.7055 - mse: 25.7055 - val_loss: 41.0085 - val_mse: 41.0085\n",
      "Epoch 73/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.6160 - mse: 25.6160 - val_loss: 40.8856 - val_mse: 40.8856\n",
      "Epoch 74/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.6277 - mse: 25.6277 - val_loss: 40.8915 - val_mse: 40.8915\n",
      "Epoch 75/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 25.7345 - mse: 25.7345 - val_loss: 40.8536 - val_mse: 40.8536\n",
      "Epoch 76/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.6937 - mse: 25.6937 - val_loss: 40.9051 - val_mse: 40.9051\n",
      "Epoch 77/100\n",
      "721/721 [==============================] - 0s 73us/step - loss: 25.7535 - mse: 25.7535 - val_loss: 40.8635 - val_mse: 40.8635\n",
      "Epoch 78/100\n",
      "721/721 [==============================] - 0s 70us/step - loss: 25.7313 - mse: 25.7313 - val_loss: 40.7044 - val_mse: 40.7044\n",
      "Epoch 79/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 26.1283 - mse: 26.1283 - val_loss: 41.0779 - val_mse: 41.0779\n",
      "Epoch 80/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 25.5403 - mse: 25.5403 - val_loss: 40.4675 - val_mse: 40.4675\n",
      "Epoch 81/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 25.6454 - mse: 25.6454 - val_loss: 40.6946 - val_mse: 40.6946\n",
      "Epoch 82/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.7142 - mse: 25.7142 - val_loss: 40.9264 - val_mse: 40.9264\n",
      "Epoch 83/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 25.4410 - mse: 25.4410 - val_loss: 40.6365 - val_mse: 40.6365\n",
      "Epoch 84/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.4276 - mse: 25.4276 - val_loss: 40.7698 - val_mse: 40.7698\n",
      "Epoch 85/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 25.4746 - mse: 25.4746 - val_loss: 40.7338 - val_mse: 40.7338\n",
      "Epoch 86/100\n",
      "721/721 [==============================] - 0s 56us/step - loss: 25.5410 - mse: 25.5410 - val_loss: 40.9743 - val_mse: 40.9743\n",
      "Epoch 87/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.4300 - mse: 25.4300 - val_loss: 40.7790 - val_mse: 40.7790\n",
      "Epoch 88/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.4134 - mse: 25.4134 - val_loss: 40.5131 - val_mse: 40.5131\n",
      "Epoch 89/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 25.6332 - mse: 25.6332 - val_loss: 40.7984 - val_mse: 40.7984\n",
      "Epoch 90/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 25.4315 - mse: 25.4315 - val_loss: 40.6608 - val_mse: 40.6608\n",
      "Epoch 91/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.4571 - mse: 25.4571 - val_loss: 40.9180 - val_mse: 40.9180\n",
      "Epoch 92/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.5068 - mse: 25.5068 - val_loss: 40.7060 - val_mse: 40.7060\n",
      "Epoch 93/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.4548 - mse: 25.4548 - val_loss: 40.5252 - val_mse: 40.5252\n",
      "Epoch 94/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.5644 - mse: 25.5644 - val_loss: 40.7640 - val_mse: 40.7640\n",
      "Epoch 95/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.4576 - mse: 25.4576 - val_loss: 40.4804 - val_mse: 40.4804\n",
      "Epoch 96/100\n",
      "721/721 [==============================] - 0s 63us/step - loss: 25.3863 - mse: 25.3863 - val_loss: 40.6904 - val_mse: 40.6904\n",
      "Epoch 97/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 25.7018 - mse: 25.7018 - val_loss: 40.6689 - val_mse: 40.6689\n",
      "Epoch 98/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 25.3113 - mse: 25.3113 - val_loss: 40.5530 - val_mse: 40.5530\n",
      "Epoch 99/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 25.3775 - mse: 25.3775 - val_loss: 40.6626 - val_mse: 40.6626\n",
      "Epoch 100/100\n",
      "721/721 [==============================] - 0s 59us/step - loss: 25.5115 - mse: 25.5115 - val_loss: 40.5763 - val_mse: 40.5763\n"
     ]
    }
   ],
   "source": [
    "# the model building and splitting are unchanged and do not need to be \n",
    "# re-written here.\n",
    "\n",
    "# fitting the model with 100 epochs\n",
    "model_norm_fit = model_norm.fit(X_train1, \n",
    "                      y_train1,\n",
    "                      validation_data = [X_test1, y_test1],\n",
    "                      epochs = 100,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43.522987]\n",
      " [46.294334]\n",
      " [17.977379]\n",
      " [37.939888]\n",
      " [46.4308  ]\n",
      " [42.125267]\n",
      " [59.85051 ]\n",
      " [23.745401]\n",
      " [63.746346]\n",
      " [20.421413]]\n",
      "\n",
      "\n",
      "Shape of array y_pred2 is: (309, 1)\n",
      "[[44.52]\n",
      " [50.53]\n",
      " [21.82]\n",
      " [38.8 ]\n",
      " [55.6 ]\n",
      " [39.42]\n",
      " [81.75]\n",
      " [24.5 ]\n",
      " [69.84]\n",
      " [19.99]]\n",
      "\n",
      "\n",
      "Shape of array y_testarray1 is: (309, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.576328711682976"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "y_pred2 = model_norm.predict(X_test1)\n",
    "\n",
    "print(y_pred2[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_pred2 is:', y_pred2.shape)\n",
    "\n",
    "# this part is repeating for the sake of simplicity.\n",
    "# reshaping array y_test1.\n",
    "y_testarray1 = np.transpose(([y_test1]))\n",
    "\n",
    "print(y_testarray1[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_testarray1 is:', y_testarray1.shape)\n",
    "\n",
    "# calculating new mean squared error\n",
    "mse = mean_squared_error(y_testarray1, y_pred2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.060368,\n",
       " 26.295803,\n",
       " 26.21351,\n",
       " 26.28386,\n",
       " 26.168173,\n",
       " 26.137217,\n",
       " 26.30051,\n",
       " 26.253283,\n",
       " 26.195427,\n",
       " 26.0047,\n",
       " 26.165976,\n",
       " 26.30013,\n",
       " 26.191015,\n",
       " 26.150616,\n",
       " 26.21371,\n",
       " 26.03956,\n",
       " 26.299803,\n",
       " 26.239338,\n",
       " 26.035156,\n",
       " 25.968206,\n",
       " 25.983742,\n",
       " 26.061834,\n",
       " 26.119642,\n",
       " 25.963696,\n",
       " 26.083628,\n",
       " 26.122208,\n",
       " 25.974592,\n",
       " 25.835775,\n",
       " 25.959957,\n",
       " 26.07628,\n",
       " 25.793644,\n",
       " 25.93254,\n",
       " 25.972265,\n",
       " 25.809645,\n",
       " 25.941063,\n",
       " 25.887323,\n",
       " 26.026222,\n",
       " 25.878353,\n",
       " 25.9667,\n",
       " 25.871153,\n",
       " 25.886118,\n",
       " 25.807646,\n",
       " 25.793905,\n",
       " 25.842705,\n",
       " 25.818548,\n",
       " 25.748423,\n",
       " 25.89999,\n",
       " 25.830122,\n",
       " 25.786093,\n",
       " 25.890858,\n",
       " 25.68104,\n",
       " 25.668407,\n",
       " 25.706446,\n",
       " 25.676046,\n",
       " 25.903254,\n",
       " 25.975744,\n",
       " 25.87215,\n",
       " 25.760069,\n",
       " 25.730541,\n",
       " 25.790602,\n",
       " 25.916996,\n",
       " 25.65252,\n",
       " 25.68079,\n",
       " 25.602427,\n",
       " 25.62966,\n",
       " 25.601004,\n",
       " 25.571983,\n",
       " 25.6308,\n",
       " 25.644222,\n",
       " 25.593365,\n",
       " 25.69668,\n",
       " 25.70552,\n",
       " 25.61599,\n",
       " 25.627657,\n",
       " 25.734453,\n",
       " 25.693684,\n",
       " 25.753502,\n",
       " 25.731346,\n",
       " 26.128296,\n",
       " 25.540285,\n",
       " 25.645412,\n",
       " 25.714153,\n",
       " 25.44098,\n",
       " 25.427593,\n",
       " 25.474564,\n",
       " 25.541008,\n",
       " 25.429998,\n",
       " 25.4134,\n",
       " 25.633152,\n",
       " 25.431467,\n",
       " 25.45711,\n",
       " 25.506805,\n",
       " 25.454775,\n",
       " 25.56437,\n",
       " 25.457554,\n",
       " 25.386347,\n",
       " 25.701817,\n",
       " 25.311314,\n",
       " 25.37746,\n",
       " 25.511475]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the mean squared error for each 50 epochs in new model\n",
    "mse_norm_data1 = model_norm_fit.history['mse']\n",
    "\n",
    "# creating the list of the 50 individual mse\n",
    "list(mse_norm_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the 100 mse with normalized data =  25.824774\n",
      "Standard deviation of the 100 mse with normalized data = 0.25524512\n"
     ]
    }
   ],
   "source": [
    "mean_norm1 = np.mean(mse_norm_data1)\n",
    "st_dev_norm1 = np.std(mse_norm_data1)\n",
    "\n",
    "print('Mean of the 100 mse with normalized data = ', mean_norm1)\n",
    "print('Standard deviation of the 100 mse with normalized data =', st_dev_norm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The increase in number of epochs greatly slightly improves the reduction in the mean and standard deviation as expected. This is, however, not a lot as expected. The algorithm performace already indicated a plateau at 50 epochs. The increase in number of hidden layers will surely be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a id = 'partD'></a>\n",
    "## Part D - increasing number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1315ab950>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the model with three hidden layers\n",
    "def regr_model_norm3():\n",
    "    model_norm3 = Sequential()\n",
    "    model_norm3.add(Dense(10, activation = 'relu', input_shape = (n_cols_norm, )))\n",
    "    model_norm3.add(Dense(10, activation = 'relu'))\n",
    "    model_norm3.add(Dense(10, activation = 'relu'))\n",
    "    model_norm3.add(Dense(10, activation = 'relu'))\n",
    "    model_norm3.add(Dense(1))\n",
    "    \n",
    "    # compiling the model\n",
    "    model_norm3.compile(optimizer = 'adam', \n",
    "                        loss = 'mean_squared_error',\n",
    "                        metrics = ['mse'])\n",
    "    \n",
    "    return model_norm3\n",
    "\n",
    "model_norm3 = regr_model_norm3()\n",
    "model_norm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/50\n",
      "721/721 [==============================] - 0s 101us/step - loss: 15.7452 - mse: 15.7452 - val_loss: 30.8306 - val_mse: 30.8306\n",
      "Epoch 2/50\n",
      "721/721 [==============================] - 0s 84us/step - loss: 16.0664 - mse: 16.0664 - val_loss: 30.7801 - val_mse: 30.7801\n",
      "Epoch 3/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 16.0723 - mse: 16.0723 - val_loss: 30.2271 - val_mse: 30.2271\n",
      "Epoch 4/50\n",
      "721/721 [==============================] - 0s 76us/step - loss: 15.9966 - mse: 15.9966 - val_loss: 30.9652 - val_mse: 30.9652\n",
      "Epoch 5/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 16.2540 - mse: 16.2540 - val_loss: 30.8616 - val_mse: 30.8616\n",
      "Epoch 6/50\n",
      "721/721 [==============================] - 0s 73us/step - loss: 16.4144 - mse: 16.4144 - val_loss: 30.2894 - val_mse: 30.2894\n",
      "Epoch 7/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 15.6262 - mse: 15.6262 - val_loss: 30.4645 - val_mse: 30.4645\n",
      "Epoch 8/50\n",
      "721/721 [==============================] - 0s 77us/step - loss: 16.5583 - mse: 16.5583 - val_loss: 32.0118 - val_mse: 32.0118\n",
      "Epoch 9/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 16.3343 - mse: 16.3343 - val_loss: 30.8187 - val_mse: 30.8187\n",
      "Epoch 10/50\n",
      "721/721 [==============================] - 0s 76us/step - loss: 15.7640 - mse: 15.7640 - val_loss: 31.5938 - val_mse: 31.5938\n",
      "Epoch 11/50\n",
      "721/721 [==============================] - 0s 82us/step - loss: 16.3199 - mse: 16.3199 - val_loss: 30.8970 - val_mse: 30.8970\n",
      "Epoch 12/50\n",
      "721/721 [==============================] - 0s 78us/step - loss: 16.0216 - mse: 16.0216 - val_loss: 30.1561 - val_mse: 30.1561\n",
      "Epoch 13/50\n",
      "721/721 [==============================] - 0s 82us/step - loss: 15.6438 - mse: 15.6438 - val_loss: 30.2192 - val_mse: 30.2192\n",
      "Epoch 14/50\n",
      "721/721 [==============================] - 0s 84us/step - loss: 15.5878 - mse: 15.5878 - val_loss: 30.7087 - val_mse: 30.7087\n",
      "Epoch 15/50\n",
      "721/721 [==============================] - 0s 85us/step - loss: 16.2117 - mse: 16.2117 - val_loss: 30.7532 - val_mse: 30.7532\n",
      "Epoch 16/50\n",
      "721/721 [==============================] - 0s 83us/step - loss: 15.7611 - mse: 15.7611 - val_loss: 30.3665 - val_mse: 30.3665\n",
      "Epoch 17/50\n",
      "721/721 [==============================] - 0s 84us/step - loss: 15.7063 - mse: 15.7063 - val_loss: 30.5568 - val_mse: 30.5569\n",
      "Epoch 18/50\n",
      "721/721 [==============================] - 0s 109us/step - loss: 15.6861 - mse: 15.6861 - val_loss: 30.3741 - val_mse: 30.3741\n",
      "Epoch 19/50\n",
      "721/721 [==============================] - 0s 80us/step - loss: 15.9463 - mse: 15.9463 - val_loss: 29.9060 - val_mse: 29.9060\n",
      "Epoch 20/50\n",
      "721/721 [==============================] - 0s 86us/step - loss: 15.4878 - mse: 15.4878 - val_loss: 30.4249 - val_mse: 30.4249\n",
      "Epoch 21/50\n",
      "721/721 [==============================] - 0s 85us/step - loss: 15.5081 - mse: 15.5081 - val_loss: 30.2219 - val_mse: 30.2219\n",
      "Epoch 22/50\n",
      "721/721 [==============================] - 0s 83us/step - loss: 15.4373 - mse: 15.4373 - val_loss: 30.5978 - val_mse: 30.5978\n",
      "Epoch 23/50\n",
      "721/721 [==============================] - 0s 71us/step - loss: 15.8698 - mse: 15.8698 - val_loss: 30.7771 - val_mse: 30.7771\n",
      "Epoch 24/50\n",
      "721/721 [==============================] - 0s 76us/step - loss: 16.0817 - mse: 16.0817 - val_loss: 31.4609 - val_mse: 31.4609\n",
      "Epoch 25/50\n",
      "721/721 [==============================] - 0s 69us/step - loss: 16.0708 - mse: 16.0708 - val_loss: 30.1187 - val_mse: 30.1187\n",
      "Epoch 26/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 15.6943 - mse: 15.6943 - val_loss: 30.2700 - val_mse: 30.2700\n",
      "Epoch 27/50\n",
      "721/721 [==============================] - 0s 70us/step - loss: 15.7288 - mse: 15.7288 - val_loss: 30.1492 - val_mse: 30.1492\n",
      "Epoch 28/50\n",
      "721/721 [==============================] - 0s 73us/step - loss: 15.8710 - mse: 15.8710 - val_loss: 30.7580 - val_mse: 30.7580\n",
      "Epoch 29/50\n",
      "721/721 [==============================] - 0s 81us/step - loss: 15.6759 - mse: 15.6759 - val_loss: 30.5208 - val_mse: 30.5208\n",
      "Epoch 30/50\n",
      "721/721 [==============================] - 0s 75us/step - loss: 15.4914 - mse: 15.4914 - val_loss: 30.6649 - val_mse: 30.6649\n",
      "Epoch 31/50\n",
      "721/721 [==============================] - 0s 119us/step - loss: 15.7572 - mse: 15.7572 - val_loss: 31.2254 - val_mse: 31.2254\n",
      "Epoch 32/50\n",
      "721/721 [==============================] - 0s 120us/step - loss: 16.1930 - mse: 16.1930 - val_loss: 32.1619 - val_mse: 32.1619\n",
      "Epoch 33/50\n",
      "721/721 [==============================] - 0s 83us/step - loss: 16.0522 - mse: 16.0522 - val_loss: 30.1030 - val_mse: 30.1030\n",
      "Epoch 34/50\n",
      "721/721 [==============================] - 0s 91us/step - loss: 15.6228 - mse: 15.6228 - val_loss: 30.1125 - val_mse: 30.1125\n",
      "Epoch 35/50\n",
      "721/721 [==============================] - 0s 92us/step - loss: 15.6463 - mse: 15.6463 - val_loss: 30.5371 - val_mse: 30.5372\n",
      "Epoch 36/50\n",
      "721/721 [==============================] - 0s 96us/step - loss: 15.6109 - mse: 15.6109 - val_loss: 30.4053 - val_mse: 30.4053\n",
      "Epoch 37/50\n",
      "721/721 [==============================] - 0s 113us/step - loss: 15.5387 - mse: 15.5387 - val_loss: 30.6324 - val_mse: 30.6324\n",
      "Epoch 38/50\n",
      "721/721 [==============================] - 0s 111us/step - loss: 15.6045 - mse: 15.6045 - val_loss: 30.4335 - val_mse: 30.4335\n",
      "Epoch 39/50\n",
      "721/721 [==============================] - 0s 86us/step - loss: 15.7530 - mse: 15.7530 - val_loss: 30.2209 - val_mse: 30.2209\n",
      "Epoch 40/50\n",
      "721/721 [==============================] - 0s 100us/step - loss: 15.6005 - mse: 15.6005 - val_loss: 31.2977 - val_mse: 31.2977\n",
      "Epoch 41/50\n",
      "721/721 [==============================] - 0s 102us/step - loss: 15.8641 - mse: 15.8641 - val_loss: 31.1382 - val_mse: 31.1382\n",
      "Epoch 42/50\n",
      "721/721 [==============================] - 0s 71us/step - loss: 15.6968 - mse: 15.6968 - val_loss: 30.5838 - val_mse: 30.5838\n",
      "Epoch 43/50\n",
      "721/721 [==============================] - 0s 71us/step - loss: 15.9118 - mse: 15.9118 - val_loss: 30.4626 - val_mse: 30.4626\n",
      "Epoch 44/50\n",
      "721/721 [==============================] - 0s 83us/step - loss: 16.0634 - mse: 16.0634 - val_loss: 30.2708 - val_mse: 30.2708\n",
      "Epoch 45/50\n",
      "721/721 [==============================] - 0s 110us/step - loss: 15.6579 - mse: 15.6579 - val_loss: 30.8132 - val_mse: 30.8132\n",
      "Epoch 46/50\n",
      "721/721 [==============================] - 0s 114us/step - loss: 15.8739 - mse: 15.8739 - val_loss: 30.2628 - val_mse: 30.2628\n",
      "Epoch 47/50\n",
      "721/721 [==============================] - 0s 90us/step - loss: 15.4486 - mse: 15.4486 - val_loss: 30.7438 - val_mse: 30.7438\n",
      "Epoch 48/50\n",
      "721/721 [==============================] - 0s 91us/step - loss: 15.5522 - mse: 15.5522 - val_loss: 30.4090 - val_mse: 30.4090\n",
      "Epoch 49/50\n",
      "721/721 [==============================] - 0s 102us/step - loss: 16.0185 - mse: 16.0185 - val_loss: 30.2306 - val_mse: 30.2306\n",
      "Epoch 50/50\n",
      "721/721 [==============================] - 0s 71us/step - loss: 15.6113 - mse: 15.6113 - val_loss: 29.9383 - val_mse: 29.9383\n"
     ]
    }
   ],
   "source": [
    "# data splitted as in Part B \n",
    "# fitting the model with 3 hidden layers and training on normalized data again\n",
    "model_norm3_fit = model_norm3.fit(X_train1, \n",
    "                      y_train1,\n",
    "                      validation_data = [X_test1, y_test1],\n",
    "                      epochs = 50,\n",
    "                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.235912]\n",
      " [51.545326]\n",
      " [18.96126 ]\n",
      " [41.100327]\n",
      " [48.8388  ]\n",
      " [40.3017  ]\n",
      " [57.238564]\n",
      " [24.380571]\n",
      " [71.77685 ]\n",
      " [16.772387]]\n",
      "\n",
      "\n",
      "Shape of array y_pred3 is: (309, 1)\n",
      "[[44.52]\n",
      " [50.53]\n",
      " [21.82]\n",
      " [38.8 ]\n",
      " [55.6 ]\n",
      " [39.42]\n",
      " [81.75]\n",
      " [24.5 ]\n",
      " [69.84]\n",
      " [19.99]]\n",
      "\n",
      "\n",
      "Shape of array y_testarray1 is: (309, 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluating model_norm3 with normalised data\n",
    "y_pred3 = model_norm3.predict(X_test1)\n",
    "\n",
    "print(y_pred3[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_pred3 is:', y_pred3.shape)\n",
    "\n",
    "# reshaping array y_test1\n",
    "y_testarray1 = np.transpose(([y_test1]))\n",
    "\n",
    "print(y_testarray1[:10]) # printing just first 10 values\n",
    "print('\\n')\n",
    "print('Shape of array y_testarray1 is:', y_testarray1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.938282695046624"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating new mean squared error with 3 layers\n",
    "mean_squared_error(y_testarray1, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.745217,\n",
       " 16.06641,\n",
       " 16.072306,\n",
       " 15.996646,\n",
       " 16.25396,\n",
       " 16.414373,\n",
       " 15.626202,\n",
       " 16.558289,\n",
       " 16.334295,\n",
       " 15.763969,\n",
       " 16.319944,\n",
       " 16.021631,\n",
       " 15.64377,\n",
       " 15.587807,\n",
       " 16.21166,\n",
       " 15.761116,\n",
       " 15.706262,\n",
       " 15.686052,\n",
       " 15.946257,\n",
       " 15.48783,\n",
       " 15.50806,\n",
       " 15.437339,\n",
       " 15.869812,\n",
       " 16.081686,\n",
       " 16.07079,\n",
       " 15.694274,\n",
       " 15.728847,\n",
       " 15.871004,\n",
       " 15.675888,\n",
       " 15.491398,\n",
       " 15.757222,\n",
       " 16.193039,\n",
       " 16.052158,\n",
       " 15.622786,\n",
       " 15.646333,\n",
       " 15.610858,\n",
       " 15.538747,\n",
       " 15.604458,\n",
       " 15.753019,\n",
       " 15.600547,\n",
       " 15.864119,\n",
       " 15.696818,\n",
       " 15.911787,\n",
       " 16.063385,\n",
       " 15.657878,\n",
       " 15.873853,\n",
       " 15.4486265,\n",
       " 15.552235,\n",
       " 16.018524,\n",
       " 15.611313]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the mean squared error for each 50 epochs in new model_norm3\n",
    "mse_norm_data3 = model_norm3_fit.history['mse']\n",
    "\n",
    "# creating the list of the 50 individual mse\n",
    "list(mse_norm_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the 50 mse of model_norm3 =  15.834216\n",
      "Standard deviation of the 50 mse of model_norm3 = 0.27055964\n"
     ]
    }
   ],
   "source": [
    "mean_norm3 = np.mean(mse_norm_data3)\n",
    "st_dev_norm3 = np.std(mse_norm_data3)\n",
    "\n",
    "print('Mean of the 50 mse of model_norm3 = ', mean_norm3)\n",
    "print('Standard deviation of the 50 mse of model_norm3 =', st_dev_norm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean is greatly reduced in this model with 3 layers, the standard deviation is unchanged. Independent of this research being done for this project, there are a number of things that can be done to further reduce the mse such as working with larger number of epochs, using better optimizer and more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
